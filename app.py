import streamlit as st
import pandas as pd
from collaborative_recommend import recommend_products as recommend_collaborative
from content_based_recommendation import recommend_products as recommend_content_based

# Function chuy·ªÉn ƒë·ªïi ƒëi·ªÉm ƒë√°nh gi√° th√†nh ng√¥i sao m√†u v√†ng.
def render_stars(rating):
            # Chuy·ªÉn ƒë·ªïi ƒëi·ªÉm ƒë√°nh gi√° th√†nh chu·ªói ng√¥i sao.
            full_star = '<span style="color: gold; font-size: 1.5em;">‚òÖ</span>'  # Ng√¥i sao v√†ng
            empty_star = '<span style="color: lightgray; font-size: 1.5em;">‚òÜ</span>'  # Ng√¥i sao tr·ªëng
            stars = full_star * int(rating) + empty_star * (5 - int(rating))
            return stars

# ƒê∆∞·ªùng d·∫´n t·ªáp
CONTENT_BASED_DATA_FILE = "data/content_based_preprocessed.csv"
COLLABORATIVE_FULL_DATA_PART1 = "data/collaborative_full_data_part1.csv"
COLLABORATIVE_FULL_DATA_PART2 = "data/collaborative_full_data_part2.csv"
COLLABORATIVE_MODEL_FILE = "model/collaborative_model.pkl.gz"

# Set page configuration
st.set_page_config(
    page_title="Hasaki Recommendation System",
    page_icon="üíÑ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Custom CSS for Hasaki-themed design
st.markdown("""
    <style>
    /* Overall page background */
    body {
        background-color: #f0f4f0;
        color: #2f6e51;
    }

    /* Sidebar styling */
    [data-testid="stSidebar"] {
        background-color: #e6f2e8;
        border-right: 2px solid #2f6e51;
    }

    /* Header and title styles */
    .header-title {
        color: #2f6e51;
        font-family: 'Arial', sans-serif;
        text-align: center;
        padding: 20px;
        background-color: #d0e5d3;
        border-radius: 10px;
        margin-bottom: 20px;
    }

    /* Banner Styling */
    .banner {
        background: url('banner/hasaki_banner_2.jpg') no-repeat center center;
        background-size: cover;
        height: 150px; /* Adjust height as needed */
        display: flex;
        justify-content: center;
        align-items: center;
        color: white;
        font-size: 32px;
        font-weight: bold;
        text-shadow: 2px 2px 5px rgba(0,0,0,0.5);
    }

    /* Layout for product display */
    .product-layout {
        display: flex;
        justify-content: space-between;
        gap: 20px;
        margin-top: 20px;
    }

    /* Style for product image */
    .product-image img {
        max-width: 100%;
        height: auto;
        border-radius: 10px;
        box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);
    }
    </style>
""", unsafe_allow_html=True)

# Sidebar th√¥ng tin nh√≥m
st.sidebar.title("Th√¥ng tin nh√≥m th·ª±c hi·ªán")
st.sidebar.write("""#### Th√†nh vi√™n th·ª±c hi·ªán:
- L√™ Qu·ª≥nh Minh Dung
- Nguy·ªÖn Th√πy Trang""")
st.sidebar.write("""#### Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n: 
- C√¥ Khu·∫•t Th√πy Ph∆∞∆°ng""")
st.sidebar.write("""#### üìà Th·ªùi gian b√°o c√°o: 16/12/2024""")

st.sidebar.title("Danh m·ª•c")
page = st.sidebar.radio(
    "L·ª±a ch·ªçn trang:", 
    ["Gi·ªõi thi·ªáu", "Quy tr√¨nh x√¢y d·ª±ng h·ªá th·ªëng", "H·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m"]
)

# Page 1: Gi·ªõi thi·ªáu
if page == "Gi·ªõi thi·ªáu":
    # Hi·ªÉn th·ªã banner
    st.title("Ch√†o m·ª´ng ƒë·∫øn v·ªõi Hasaki.vn!")
    
    col1, col2 = st.columns([1, 3])  # T·∫°o hai c·ªôt, c·ªôt ƒë·∫ßu nh·ªè h∆°n ƒë·ªÉ ch·ª©a logo

    # C·ªôt 1: Hi·ªÉn th·ªã logo
    with col1:
        st.image("banner/Logo.png", use_column_width=True, width=100, caption="")

    # C·ªôt 2: Hi·ªÉn th·ªã ph·∫ßn "V·ªÅ Hasaki"
    with col2:
        st.subheader("üíÑ V·ªÅ **`Hasaki.vn`**")
        st.write("""
            Hasaki.vn cam k·∫øt mang ƒë·∫øn nh·ªØng s·∫£n ph·∫©m l√†m ƒë·∫πp v√† chƒÉm s√≥c da t·ªët nh·∫•t cho kh√°ch h√†ng.
            V·ªõi tr·ªçng t√¢m l√† ch·∫•t l∆∞·ª£ng v√† s·ª± h√†i l√≤ng c·ªßa kh√°ch h√†ng, Hasaki h∆∞·ªõng ƒë·∫øn vi·ªác l√†m ƒë·∫πp tr·ªü n√™n d·ªÖ ti·∫øp c·∫≠n v·ªõi m·ªçi ng∆∞·ªùi.
        """)

    st.subheader("üéØ M·ª•c ti√™u ch√≠nh")
    st.write("""
        H·ªá th·ªëng g·ª£i √Ω (Recommend system) c·ªßa ch√∫ng t√¥i ƒë∆∞·ª£c thi·∫øt k·∫ø nh·∫±m:
        - üõçÔ∏è Gi√∫p kh√°ch h√†ng c·ªßa ch√∫ng t√¥i kh√°m ph√° c√°c s·∫£n ph·∫©m ph√π h·ª£p v·ªõi s·ªü th√≠ch c·ªßa m√¨nh.
        - üí° C·∫£i thi·ªán tr·∫£i nghi·ªám mua s·∫Øm b·∫±ng c√°ch g·ª£i √Ω c√°c s·∫£n ph·∫©m li√™n quan.
        - üìä ·ª®ng d·ª•ng c√°c thu·∫≠t to√°n ti√™n ti·∫øn nh∆∞ **`Content-Based Filtering`** v√† **`Collaborative Filtering`** ƒë·ªÉ mang l·∫°i g·ª£i √Ω c√° nh√¢n h√≥a.
    """)
    st.markdown("<br>", unsafe_allow_html=True)  # Th√™m kho·∫£ng tr·ªëng
    st.image("banner/images.png", use_column_width=True, width=800, caption="Mang ƒë·∫øn tr·∫£i nghi·ªám l√†m ƒë·∫πp tuy·ªát v·ªùi")

# Page 2: Quy tr√¨nh x√¢y d·ª±ng h·ªá th·ªëng
elif page == "Quy tr√¨nh x√¢y d·ª±ng h·ªá th·ªëng":
    st.markdown(f'''
    <div class="header-title">
        <h1 style="color: #2f6e51; margin-bottom: 10px; text-align: center;">üåø Quy tr√¨nh x√¢y d·ª±ng<br>Recommendation System</h1>
    </div>
    ''', unsafe_allow_html=True)
    st.write("""
        Quy tr√¨nh x√¢y d·ª±ng h·ªá th·ªëng g·ª£i √Ω t·∫°i Hasaki.vn ƒë∆∞·ª£c chia th√†nh hai ph∆∞∆°ng ph√°p ch√≠nh:
        1. **Content-Based Filtering**: G·ª£i √Ω s·∫£n ph·∫©m d·ª±a tr√™n n·ªôi dung m√¥ t·∫£ c·ªßa s·∫£n ph·∫©m.
        2. **Collaborative Filtering**: G·ª£i √Ω s·∫£n ph·∫©m d·ª±a tr√™n h√†nh vi c·ªßa kh√°ch h√†ng kh√°c.
    """)

    tab1, tab2, tab3 = st.tabs(["Crawl Data", "Content-Based Filtering", "Collaborative Filtering"])

    # Tab Crawl Data
    with tab1:
        st.title("Crawl Data t·ª´ Hasaki.vn")

        with st.expander("Code c√†o d·ªØ li·ªáu (Click ƒë·ªÉ m·ªü)"):
            st.code("""
    from bs4 import BeautifulSoup
    import requests
    import pandas as pd
    import time

    # URL c·ªßa trang web "ChƒÉm s√≥c da m·∫∑t" tr√™n Hasaki.vn
    BASE_URL = "https://hasaki.vn/danh-muc/cham-soc-da-mat-c4.html"

    # H√†m c√†o d·ªØ li·ªáu ch√≠nh
    def scrape_hasaki_data(base_url, num_pages=5):
        products = []
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36",
            "Accept-Language": "en-US,en;q=0.9",
        }

        for page in range(1, num_pages + 1):
            url = f"{base_url}?p={page}"  # C·∫≠p nh·∫≠t tham s·ªë trang
            print(f"ƒêang c√†o d·ªØ li·ªáu t·ª´: {url}")
            try:
                response = requests.get(url, headers=headers)
                if response.status_code != 200:
                    print(f"Kh√¥ng th·ªÉ truy c·∫≠p trang: {url} (status code: {response.status_code})")
                    continue
                
                soup = BeautifulSoup(response.content, 'html.parser')
                product_items = soup.find_all('div', class_='item_sp_hasaki width_common relative')

                for item in product_items:
                    try:
                        # L·∫•y th√¥ng tin t·ª´ HTML
                        product_link_tag = item.find('a', class_='block_info_item_sp width_common card-body')
                        product_id = product_link_tag['data-id'] if product_link_tag and 'data-id' in product_link_tag.attrs else "Kh√¥ng c√≥ m√£ s·∫£n ph·∫©m"
                        product_name = product_link_tag['data-name'] if product_link_tag and 'data-name' in product_link_tag.attrs else "Kh√¥ng c√≥ t√™n s·∫£n ph·∫©m"
                        price = item.find('strong', class_='item_giamoi txt_16').text.strip().replace(" ‚Ç´", "").replace(".", "") if item.find('strong', class_='item_giamoi txt_16') else "Kh√¥ng c√≥ gi√° b√°n"
                        original_price = item.find('span', class_='item_giacu txt_12 right').text.strip().replace(" ‚Ç´", "").replace(".", "") if item.find('span', class_='item_giacu txt_12 right') else "Kh√¥ng c√≥ gi√° g·ªëc"
                        image_url = item.find('img', class_='img_thumb lazy')['data-src'] if item.find('img', class_='img_thumb lazy') else "Kh√¥ng c√≥ h√¨nh ·∫£nh"

                        # Th√™m s·∫£n ph·∫©m v√†o danh s√°ch
                        products.append({
                            "ma_san_pham": product_id,
                            "ten_san_pham": product_name,
                            "gia_ban": price,
                            "gia_goc": original_price,
                            "hinh_anh": image_url,
                        })
                    except Exception as e:
                        print(f"L·ªói khi x·ª≠ l√Ω s·∫£n ph·∫©m: {e}")

                time.sleep(2)  # Ngh·ªâ 2 gi√¢y gi·ªØa c√°c y√™u c·∫ßu

            except Exception as e:
                print(f"L·ªói khi truy c·∫≠p: {url}, {e}")
        
        return products

        # G·ªçi h√†m c√†o d·ªØ li·ªáu
        data = scrape_hasaki_data(BASE_URL, num_pages=68)

        # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√†nh DataFrame
        df = pd.DataFrame(data)

        # Lo·∫°i b·ªè s·∫£n ph·∫©m tr√πng l·∫∑p d·ª±a tr√™n 'ma_san_pham'
        df = df.drop_duplicates(subset=['ma_san_pham'], keep='first')

        # L∆∞u th√†nh file CSV
        df.to_csv('San_pham_new.csv', index=False, encoding='utf-8-sig')
        print("D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o file San_pham_new.csv!")
                """, language="python")

        st.write("""
            **Gi·∫£i th√≠ch:**
            - **M·ª•c ƒë√≠ch:** C√†o d·ªØ li·ªáu s·∫£n ph·∫©m t·ª´ danh m·ª•c "ChƒÉm s√≥c da m·∫∑t" tr√™n trang Hasaki.vn.
            - **Chi ti·∫øt:** 
                1. **`BeautifulSoup`**: Th∆∞ vi·ªán ƒë·ªÉ ph√¢n t√≠ch c√∫ ph√°p HTML.
                2. **D·ªØ li·ªáu c√†o ƒë∆∞·ª£c:**
                    - **`ma_san_pham`**: M√£ ƒë·ªãnh danh s·∫£n ph·∫©m.
                    - **`ten_san_pham`**: T√™n s·∫£n ph·∫©m.
                    - **`gia_ban`**: Gi√° hi·ªán t·∫°i c·ªßa s·∫£n ph·∫©m.
                    - **`gia_goc`**: Gi√° g·ªëc (n·∫øu c√≥).
                    - **`hinh_anh`**: URL h√¨nh ·∫£nh s·∫£n ph·∫©m.
                3. **Lo·∫°i b·ªè tr√πng l·∫∑p:** S·∫£n ph·∫©m ƒë∆∞·ª£c l·ªçc d·ª±a tr√™n `ma_san_pham`.
                4. **ƒê·∫ßu ra:** L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o file **`San_pham_new.csv`** ƒë·ªÉ merge chung v·ªõi file g·ªëc l√† **`San_pham.csv`** d·ª±a tr√™n c·ªôt `ma_san_pham`.
                5. **Th·ªùi gian ngh·ªâ (sleep):** Gi·ªØa m·ªói l·∫ßn c√†o m·ªôt trang, ch∆∞∆°ng tr√¨nh ngh·ªâ 2 gi√¢y ƒë·ªÉ tr√°nh b·ªã ch·∫∑n.
        """)

        st.write("### D·ªØ li·ªáu m·∫´u c√†o ƒë∆∞·ª£c:")
        # Hi·ªÉn th·ªã d·ªØ li·ªáu m·∫´u
        try:
            df_sample = pd.read_csv('data/San_pham_new.csv').head()
            df_sample['ma_san_pham'] = df_sample['ma_san_pham'].astype(str)
            
            # ƒê·ªãnh d·∫°ng gi√° b√°n v√† gi√° g·ªëc
            for col in ['gia_ban', 'gia_goc']:
                if col in df_sample.columns:
                    df_sample[col] = df_sample[col].apply(
                        lambda x: f"{int(x):,}" if pd.notnull(x) and str(x).isdigit() else x
                    )

            # Hi·ªÉn th·ªã d·ªØ li·ªáu ƒë√£ ƒë·ªãnh d·∫°ng
            st.dataframe(df_sample)
        except FileNotFoundError:
            st.warning("Kh√¥ng t√¨m th·∫•y file `San_pham_new.csv`. Vui l√≤ng ch·∫°y m√£ c√†o d·ªØ li·ªáu tr∆∞·ªõc.")

    # Tab Content-Based Filtering
    with tab2:
        # Streamlit layout
        st.title("Content-Based Filtering: Quy tr√¨nh x√¢y d·ª±ng v√† ph√¢n t√≠ch")

        st.markdown("""
        1. **ƒê·ªçc d·ªØ li·ªáu t·ª´ c√°c t·ªáp CSV:**
            - **San_pham.csv:** Ch·ª©a th√¥ng tin s·∫£n ph·∫©m (t√™n, m√¥ t·∫£, gi√°, ƒëi·ªÉm ƒë√°nh gi√°...).
            - **Danh_gia.csv:** Ch·ª©a ƒë√°nh gi√° c·ªßa kh√°ch h√†ng.
        2. **L√†m s·∫°ch d·ªØ li·ªáu v√† ti·ªÅn x·ª≠ l√Ω:**
            - Lo·∫°i b·ªè stopwords, k√Ω t·ª± ƒë·∫∑c bi·ªát (Tokenize).
            - K·∫øt h·ª£p t√™n s·∫£n ph·∫©m, m√¥ t·∫£ v√† ph√¢n lo·∫°i ƒë·ªÉ t·∫°o n·ªôi dung phong ph√∫ h∆°n.
            - T√≠nh **`so_sao_trung_binh`** cho m·ªói s·∫£n ph·∫©m b·∫±ng c√°ch nh√≥m theo **`ma_san_pham`** t·ª´ b·∫£ng **`Danh_gia.csv`**.
            - G·ªôp th√¥ng tin **`so_sao_trung_binh`** v√†o file **`San_pham.csv`** ƒë·ªÉ t·∫°o m·ªôt b·∫£ng t·ªïng h·ª£p.
        3. **Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng:**
            - S·ª≠ d·ª•ng **`Gensim`** v·ªõi **`TF-IDF`** ƒë·ªÉ t√≠nh to√°n m·ª©c ƒë·ªô quan tr·ªçng c·ªßa t·ª´ trong m√¥ t·∫£ s·∫£n ph·∫©m.
            - T·∫°o ma tr·∫≠n t∆∞∆°ng t·ª± (**`ma tr·∫≠n sparse`**) d·ª±a tr√™n n·ªôi dung.
        4. **T√≠nh to√°n ƒë·∫∑c tr∆∞ng:**
            - T√≠nh to√°n ƒëi·ªÉm t·ªïng h·ª£p cho t·ª´ng s·∫£n ph·∫©m b·∫±ng c√°ch k·∫øt h·ª£p ƒëi·ªÉm t∆∞∆°ng t·ª± n·ªôi dung (**`similarity score`**) v√† ƒëi·ªÉm ƒë√°nh gi√° trung b√¨nh (**`average rating`**). D√πng ƒëi·ªÉm t·ªïng h·ª£p n√†y ƒë·ªÉ x√°c ƒë·ªãnh c√°c s·∫£n ph·∫©m t∆∞∆°ng t·ª± nh·∫•t nh∆∞ng v·∫´n ƒë·∫£m b·∫£o ∆∞u ti√™n c√°c s·∫£n ph·∫©m c√≥ ƒë√°nh gi√° t·ªët h∆°n.
            - S·∫Øp x·∫øp v√† tr·∫£ v·ªÅ c√°c s·∫£n ph·∫©m ph√π h·ª£p nh·∫•t.
        """)

        # Load d·ªØ li·ªáu
        san_pham_path = "data/san_pham_updated.csv"
        san_pham_preprocessed_path = "data/content_based_preprocessed.csv"
        san_pham_df = pd.read_csv(san_pham_path)
        san_pham_df['ma_san_pham'] = san_pham_df['ma_san_pham'].astype(str)
        san_pham_preprocessed_df = pd.read_csv(san_pham_preprocessed_path)
        san_pham_preprocessed_df['ma_san_pham'] = san_pham_preprocessed_df['ma_san_pham'].astype(str)

        # Display raw data
        st.write("### D·ªØ li·ªáu g·ªëc:")
        st.dataframe(san_pham_df[['ma_san_pham', 'ten_san_pham', 'mo_ta', 'diem_trung_binh']].head())

        # Display tokenized data
        st.write("### D·ªØ li·ªáu sau khi ti·ªÅn x·ª≠ l√Ω:")
        st.dataframe(san_pham_preprocessed_df[['ma_san_pham', 'processed_content', 'tokens', 'token_count']].head())

        # Token distribution
        st.write("### Ph√¢n ph·ªëi s·ªë l∆∞·ª£ng t·ª´ trong m√¥ t·∫£ s·∫£n ph·∫©m:")
        st.image("banner/distribution.png", use_column_width=False, width=650, caption="")

        st.markdown("""
        **Nh·∫≠n x√©t:**
        - ƒê·ªô d√†i m√¥ t·∫£ t·∫≠p trung trong kho·∫£ng 100-150 t·ª´, cho th·∫•y m√¥ t·∫£ s·∫£n ph·∫©m ·ªü m·ª©c ƒë·ªô v·ª´a ph·∫£i, kh√¥ng qu√° d√†i ho·∫∑c ng·∫Øn.
        - M·ªôt s·ªë s·∫£n ph·∫©m c√≥ m√¥ t·∫£ r·∫•t ng·∫Øn (<100 tokens) ho·∫∑c r·∫•t d√†i (>300 tokens), c√≥ th·ªÉ c·∫ßn ƒë∆∞·ª£c chu·∫©n h√≥a.
        - Ph√¢n ph·ªëi ƒë·ªÅu, kh√¥ng c√≥ s·ª± l·ªách ƒë√°ng k·ªÉ.
        """)

        # Relationship between token count and average rating
        st.write("### Quan h·ªá gi·ªØa ƒëi·ªÉm ƒë√°nh gi√° v√† ƒë·ªô d√†i m√¥ t·∫£:")
        st.image("banner/relationship.png", use_column_width=False, width=650, caption="")

        st.markdown("""
        **Nh·∫≠n x√©t:**
        - Kh√¥ng c√≥ m·ªëi quan h·ªá tuy·∫øn t√≠nh r√µ r√†ng gi·ªØa ƒë·ªô d√†i m√¥ t·∫£ (token_count) v√† ƒëi·ªÉm ƒë√°nh gi√° trung b√¨nh (diem_trung_binh).
        - C√°c s·∫£n ph·∫©m c√≥ ƒëi·ªÉm cao (4-5) xu·∫•t hi·ªán ·ªü nhi·ªÅu m·ª©c token, t·ª´ ng·∫Øn ƒë·∫øn d√†i.
        - C√°c s·∫£n ph·∫©m c√≥ ƒëi·ªÉm b·∫±ng 0 tr·∫£i r·ªông ·ªü m·ªçi ƒë·ªô d√†i m√¥ t·∫£, c·∫ßn ki·ªÉm tra l·∫°i d·ªØ li·ªáu.
        """)

        # Correlation heatmap
        st.write("### Heatmap t∆∞∆°ng quan:")
        st.image("banner/heatmap.png", use_column_width=False, width=650, caption="")

        st.markdown("""
        **Nh·∫≠n x√©t:**
        - T∆∞∆°ng quan gi·ªØa diem_trung_binh v√† token_count l√† -0.02 (g·∫ßn 0), cho th·∫•y ƒë·ªô d√†i m√¥ t·∫£ kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn ƒëi·ªÉm ƒë√°nh gi√°.
        - C·∫ßn ph√¢n t√≠ch th√™m c√°c y·∫øu t·ªë kh√°c (gi√° c·∫£, lo·∫°i s·∫£n ph·∫©m, h√¨nh ·∫£nh) ƒë·ªÉ t√¨m m·ªëi quan h·ªá c√≥ √Ω nghƒ©a h∆°n.
        """)

        # Top 10 most frequent words
        st.write("### Top 10 t·ª´ ph·ªï bi·∫øn nh·∫•t trong m√¥ t·∫£ s·∫£n ph·∫©m:")
        st.image("banner/frequent_words.png", use_column_width=False, width=650, caption="")

        st.markdown("""
        **Nh·∫≠n x√©t:**
        - "h√†ng", "ƒë∆°n", "da", "h√≥a": ƒê√¢y l√† c√°c t·ª´ ph·ªï bi·∫øn nh∆∞ng mang t√≠nh t·ªïng qu√°t v√† th∆∞·ªùng kh√¥ng cung c·∫•p nhi·ªÅu th√¥ng tin ƒë·∫∑c tr∆∞ng cho s·∫£n ph·∫©m.
        - "ƒë·ªè", "hasaki": C√°c t·ª´ n√†y c√≥ th·ªÉ l√† ƒë·∫∑c tr∆∞ng c·ªßa s·∫£n ph·∫©m (m√†u s·∫Øc ho·∫∑c th∆∞∆°ng hi·ªáu) nh∆∞ng c·∫ßn ki·ªÉm tra xem nh·ªØng t·ª´ n√†y c√≥ xu·∫•t hi·ªán qu√° th∆∞·ªùng xuy√™n v√† c√≥ gi√° tr·ªã ph√¢n t√≠ch hay kh√¥ng.
        - "kh√¥ng", "xu·∫•t", "h": ƒê√¢y l√† nh·ªØng t·ª´ c√≥ kh·∫£ nƒÉng kh√¥ng mang l·∫°i √Ω nghƒ©a ƒë·∫∑c bi·ªát cho m√¥ t·∫£ s·∫£n ph·∫©m, ƒë·∫∑c bi·ªát t·ª´ nh∆∞ "h" ho·∫∑c "kh√¥ng" c√≥ th·ªÉ b·ªã xem l√† stopwords.
        """)

        # Ph·∫ßn ch·ªçn model
        st.write("### L·ª±a ch·ªçn model cho Content-Based Filtering")

        # M√¥ t·∫£ ch·ªçn model
        st.markdown("""
        ƒê·ªÉ x√¢y d·ª±ng m√¥ h√¨nh Content-Based Filtering, ch√∫ng t√¥i ƒë√£ th·ª≠ nghi·ªám v√† so s√°nh gi·ªØa hai ph∆∞∆°ng ph√°p ch√≠nh:

        1. **Gensim (TF-IDF):** 
            - S·ª≠ d·ª•ng TF-IDF ƒë·ªÉ vector h√≥a n·ªôi dung m√¥ t·∫£ s·∫£n ph·∫©m.
            - T√≠nh to√°n m·ª©c ƒë·ªô t∆∞∆°ng t·ª± d·ª±a tr√™n ma tr·∫≠n sparse.
        2. **Cosine Similarity:**
            - Vector h√≥a m√¥ t·∫£ s·∫£n ph·∫©m b·∫±ng Bag-of-Words (BOW).
            - T√≠nh to√°n m·ª©c ƒë·ªô t∆∞∆°ng t·ª± gi·ªØa c√°c s·∫£n ph·∫©m b·∫±ng Cosine Similarity.
        """)

        # Th√™m ƒë√°nh gi√° ∆∞u, nh∆∞·ª£c ƒëi·ªÉm
        st.write("### ƒê√°nh gi√° gi·ªØa c√°c ph∆∞∆°ng ph√°p")
        st.markdown("""
        | **Model**            | **∆Øu ƒëi·ªÉm**                                                                                   | **Nh∆∞·ª£c ƒëi·ªÉm**                                                                 |
        |-----------------------|-----------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------|
        | **Gensim**            | - T·ªëi ∆∞u tr√™n d·ªØ li·ªáu l·ªõn nh·ªù TF-IDF v√† ma tr·∫≠n sparse.                                       | - ƒê·ªô ƒëa d·∫°ng g·ª£i √Ω th·∫•p.                                                       |
        |                       | - K·∫øt h·ª£p t·ªët gi·ªØa n·ªôi dung v√† ƒëi·ªÉm ƒë√°nh gi√° trung b√¨nh.                                      | - Y√™u c·∫ßu ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu t·ªët ƒë·ªÉ ƒë·∫°t hi·ªáu qu·∫£.                              |
        | **Cosine Similarity** | - Nhanh h∆°n v√† ph√π h·ª£p tr√™n t·∫≠p d·ªØ li·ªáu nh·ªè ho·∫∑c trung b√¨nh (<10,000 s·∫£n ph·∫©m).               | - Hi·ªáu su·∫•t gi·∫£m tr√™n t·∫≠p d·ªØ li·ªáu l·ªõn do t√≠nh to√°n to√†n b·ªô ma tr·∫≠n t∆∞∆°ng t·ª±.   |
        |                       | - ƒê·ªô bao ph·ªß v√† ƒëa d·∫°ng s·∫£n ph·∫©m g·ª£i √Ω t·ªët h∆°n Gensim.                                        | - Ph·ª• thu·ªôc nhi·ªÅu v√†o vector h√≥a n·ªôi dung, kh√¥ng ph√¢n bi·ªát tr·ªçng s·ªë t·ª´ quan tr·ªçng. |
        """)

        st.markdown("""
        **L·ª±a ch·ªçn:** D·ª±a tr√™n ƒë√°nh gi√°, ch√∫ng t√¥i ch·ªçn **Gensim** v√¨ kh·∫£ nƒÉng t·ªëi ∆∞u tr√™n d·ªØ li·ªáu l·ªõn (>10,000 s·∫£n ph·∫©m) v√† hi·ªáu qu·∫£ cao trong vi·ªác k·∫øt h·ª£p ƒëi·ªÉm t∆∞∆°ng t·ª± n·ªôi dung v·ªõi ƒëi·ªÉm ƒë√°nh gi√° trung b√¨nh.
        """)


        # Tab Collaborative Filtering
        with tab3:
            # Ti√™u ƒë·ªÅ
            st.title("Collaborative Filtering: Quy tr√¨nh x√¢y d·ª±ng v√† ph√¢n t√≠ch")

            st.markdown("""
            1. **X·ª≠ l√Ω d·ªØ li·ªáu:**
                - ƒê·ªçc d·ªØ li·ªáu t·ª´ file ƒë√°nh gi√° kh√°ch h√†ng (**`Danh_gia.csv`**).
                - Lo·∫°i b·ªè d·ªØ li·ªáu tr√πng l·∫∑p d·ª±a tr√™n kh√°ch h√†ng, s·∫£n ph·∫©m v√† s·ªë sao.
                - L·ªçc gi·ªØ l·∫°i ƒë√°nh gi√° g·∫ßn nh·∫•t cho m·ªói kh√°ch h√†ng v√† s·∫£n ph·∫©m.
                - L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o file m·ªõi.
            2. **Ph√¢n t√≠ch d·ªØ li·ªáu:**
                - Kh√°m ph√° ph√¢n ph·ªëi s·ªë sao, s·ªë l∆∞·ª£t ƒë√°nh gi√° tr√™n m·ªói s·∫£n ph·∫©m v√† m·ªói kh√°ch h√†ng.
                - L·ªçc c√°c s·∫£n ph·∫©m v√† ng∆∞·ªùi d√πng c√≥ √≠t ƒë√°nh gi√° ƒë·ªÉ gi·∫£m t√≠nh ƒëa chi·ªÅu.
            3. **Hu·∫•n luy·ªán m√¥ h√¨nh:**
                - S·ª≠ d·ª•ng thu·∫≠t to√°n **`KNNBaseline`** t·ª´ th∆∞ vi·ªán **`Surprise`** ƒë·ªÉ x√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n ƒë√°nh gi√° sao c·ªßa kh√°ch h√†ng cho c√°c s·∫£n ph·∫©m ch∆∞a ƒë√°nh gi√°.
                - L∆∞u m√¥ h√¨nh **`collaborative_model.pkl.gz`** ƒë√£ hu·∫•n luy·ªán v√†o file ƒë·ªÉ s·ª≠ d·ª•ng cho g·ª£i √Ω.
            4. **G·ª£i √Ω s·∫£n ph·∫©m:**
                - D·ª±a tr√™n m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán, d·ª± ƒëo√°n ƒëi·ªÉm s·ªë v√† ƒë·ªÅ xu·∫•t c√°c s·∫£n ph·∫©m ph√π h·ª£p.
            """)

            # ƒê·ªçc d·ªØ li·ªáu
            raw_data = pd.read_csv("data/Danh_gia.csv")
            raw_data['ma_khach_hang'] = raw_data['ma_khach_hang'].astype(str)
            raw_data['ma_san_pham'] = raw_data['ma_san_pham'].astype(str)
            processed_data = pd.read_csv("data/collaborative_full_data_part1.csv")
            processed_data['id'] = processed_data['id'].astype(str)
            processed_data['ma_khach_hang'] = processed_data['ma_khach_hang'].astype(str)
            processed_data['ma_san_pham'] = processed_data['ma_san_pham'].astype(str)

            # Hi·ªÉn th·ªã d·ªØ li·ªáu tr∆∞·ªõc x·ª≠ l√Ω
            st.write("#### D·ªØ li·ªáu g·ªëc:")
            st.dataframe(raw_data.head())

            # Hi·ªÉn th·ªã d·ªØ li·ªáu sau x·ª≠ l√Ω
            st.write("#### D·ªØ li·ªáu sau x·ª≠ l√Ω:")
            st.dataframe(processed_data.head())

            # Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi s·ªë sao
            st.write("### Ph√¢n ph·ªëi s·ªë sao:")
            st.image("banner/star_distribution.png", use_column_width=False, width=650, caption="")

            st.markdown("""
            **Nh·∫≠n x√©t:**
            - H∆°n 90% s·ªë l∆∞·ª£t ƒë√°nh gi√° l√† t√≠ch c·ª±c (t·ª´ 4 sao tr·ªü l√™n).
            - C√°c s·∫£n ph·∫©m c√≥ ƒë√°nh gi√° th·∫•p chi·∫øm t·ª∑ l·ªá r·∫•t nh·ªè.
            """)

            # Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi s·ªë l∆∞·ª£t ƒë√°nh gi√° tr√™n m·ªói s·∫£n ph·∫©m
            st.write("### Ph√¢n ph·ªëi s·ªë l∆∞·ª£t ƒë√°nh gi√° tr√™n m·ªói s·∫£n ph·∫©m:")
            st.image("banner/product_distribution.png", use_column_width=False, width=650, caption="")

            st.markdown("""
            **Nh·∫≠n x√©t:**
            - ƒêa ph·∫ßn c√°c s·∫£n ph·∫©m c√≥ s·ªë l∆∞·ª£ng ƒë√°nh gi√° d∆∞·ªõi 30 l∆∞·ª£t.
            - M·ªôt s·ªë √≠t s·∫£n ph·∫©m ƒë∆∞·ª£c ƒë√°nh gi√° r·∫•t nhi·ªÅu (~ 5 s·∫£n ph·∫©m), l√™n ƒë·∫øn 300 l∆∞·ª£t.
            """)

            # Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi s·ªë l∆∞·ª£t ƒë√°nh gi√° tr√™n m·ªói kh√°ch h√†ng
            st.write("### Ph√¢n ph·ªëi s·ªë l∆∞·ª£t ƒë√°nh gi√° tr√™n m·ªói kh√°ch h√†ng:")
            st.image("banner/customer_distribution.png", use_column_width=False, width=650, caption="")

            st.markdown("""
            **Nh·∫≠n x√©t:**
            - ƒêa ph·∫ßn kh√°ch h√†ng ƒë√°nh gi√° d∆∞·ªõi 25 l∆∞·ª£t.
            - M·ªôt s·ªë kh√°ch h√†ng t√≠ch c·ª±c ƒë√°nh gi√° l√™n ƒë·∫øn 70 l∆∞·ª£t.
            """)

            # Ph·∫ßn ch·ªçn thu·∫≠t to√°n
            st.write("### L·ª±a ch·ªçn thu·∫≠t to√°n cho m√¥ h√¨nh Surprise")

            # Th√™m m√¥ t·∫£
            st.markdown("""
            ƒê·ªÉ l·ª±a ch·ªçn thu·∫≠t to√°n t·ªëi ∆∞u nh·∫•t cho m√¥ h√¨nh g·ª£i √Ω, ch√∫ng t√¥i ƒë√£ th·ª±c hi·ªán ch·∫°y v√† ƒë√°nh gi√° **11 thu·∫≠t to√°n kh√°c nhau** d·ª±a tr√™n ch·ªâ s·ªë **RMSE (Root Mean Square Error)**. 
            K·∫øt qu·∫£ cho th·∫•y, c√°c thu·∫≠t to√°n thu·ªôc nh√≥m **KNN** ƒë·∫°t ch·ªâ s·ªë RMSE th·∫•p nh·∫•t (kho·∫£ng **0.55**) so v·ªõi c√°c thu·∫≠t to√°n c√≤n l·∫°i.

            V√¨ v·∫≠y, ch√∫ng t√¥i ch·ªçn s·ª≠ d·ª•ng **KNNBaseline**, v√¨ thu·∫≠t to√°n n√†y kh√¥ng ch·ªâ ƒë·∫°t hi·ªáu qu·∫£ cao m√† c√≤n ph√π h·ª£p v·ªõi d·ªØ li·ªáu c·ªßa Hasaki.
            """)

            # Hi·ªÉn th·ªã h√¨nh ·∫£nh ƒë√°nh gi√° RMSE c·ªßa c√°c thu·∫≠t to√°n
            st.image("banner/model.png", use_column_width=False, width=550, caption="So s√°nh RMSE gi·ªØa c√°c thu·∫≠t to√°n")

            # So s√°nh gi·ªØa ALS v√† Surprise
            st.write("### ƒê√°nh gi√° l·ª±a ch·ªçn gi·ªØa ALS v√† Surprise")
            st.markdown("""
            ƒê·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh gi·ªØa **ALS** v√† **Surprise**, ch√∫ng t√¥i so s√°nh d·ª±a tr√™n ba ti√™u ch√≠:

            | **Ti√™u ch√≠**        | **ALS**                                     | **Surprise**                             |
            |----------------------|---------------------------------------------|------------------------------------------|
            | **M·ª•c ƒë√≠ch**        | Ph√¢n t√≠ch ma tr·∫≠n, t·ªëi ∆∞u cho d·ªØ li·ªáu l·ªõn.  | Th·ª≠ nghi·ªám nhanh c√°c thu·∫≠t to√°n g·ª£i √Ω.   |
            | **Hi·ªáu su·∫•t**       | Ph√π h·ª£p h∆°n tr√™n d·ªØ li·ªáu l·ªõn, th∆∞a.         | Ph√π h·ª£p v·ªõi d·ªØ li·ªáu v·ª´a v√† nh·ªè.          |
            | **RMSE**            | 0.697868                                   | **0.556424**                            |

            **K·∫øt lu·∫≠n:** V·ªõi t·∫≠p d·ªØ li·ªáu hi·ªán t·∫°i, **Surprise** l√† l·ª±a ch·ªçn t·ªëi ∆∞u h∆°n do ch·ªâ s·ªë **RMSE** th·∫•p h∆°n, v√† kh·∫£ nƒÉng tri·ªÉn khai nhanh c√°c thu·∫≠t to√°n nh∆∞ **KNNBaseline**.
            """)

# Page 3: H·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m
elif page == "H·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m":
    # Hi·ªÉn th·ªã banner
    banner_path = "banner/hasaki_banner.png"  # ƒê∆∞·ªùng d·∫´n c·ª•c b·ªô

    st.image(
        banner_path,
        use_column_width=True,  # T·ª± ƒë·ªông cƒÉn ch·ªânh theo chi·ªÅu r·ªông c·ªßa trang
        caption=None  # Kh√¥ng hi·ªÉn th·ªã ch√∫ th√≠ch
    )
    # Hi·ªÉn th·ªã ti√™u ƒë·ªÅ ch√≠nh
    st.title("Hasaki g·ª£i √Ω s·∫£n ph·∫©m cho b·∫°n")

    # Tabs ƒë·ªÉ ch·ªçn gi·ªØa hai ph∆∞∆°ng ph√°p g·ª£i √Ω
    tab1, tab2 = st.tabs(["Content-Based Filtering", "Collaborative Filtering"])

    # Tab 1: Content-Based Filtering
    with tab1:
        # ƒê·ªçc d·ªØ li·ªáu s·∫£n ph·∫©m
        df_products = pd.read_csv(CONTENT_BASED_DATA_FILE)
        df_products['ten_san_pham'] = df_products['ten_san_pham'].astype(str).fillna('')
        df_products['ma_san_pham'] = df_products['ma_san_pham'].astype(str).fillna('')
        df_products['hinh_anh'] = df_products['hinh_anh'].astype(str).fillna("https://via.placeholder.com/150")

        # Ch·ªçn t√™n s·∫£n ph·∫©m
        product_names = df_products['ten_san_pham'].unique()
        selected_product_name = st.selectbox(
            "Nh·∫≠p ho·∫∑c ch·ªçn t√™n s·∫£n ph·∫©m y√™u th√≠ch:",
            options=[""] + list(product_names),
            format_func=lambda x: x if x else "Ch·ªçn s·∫£n ph·∫©m",
            key="product_name"
        )

        # L·ªçc s·∫£n ph·∫©m d·ª±a tr√™n l·ª±a ch·ªçn
        filtered_products = df_products[df_products['ten_san_pham'] == selected_product_name] if selected_product_name else pd.DataFrame()
        
        # Hi·ªÉn th·ªã s·∫£n ph·∫©m ƒë√£ ch·ªçn
        if not filtered_products.empty:
            selected_product_data = filtered_products.iloc[0]
            st.markdown("---")
            st.markdown(f"<h2 style='color: #2E8B57; text-align: center;'>{selected_product_data['ten_san_pham']}</h2>", unsafe_allow_html=True)
            
            # T·∫°o b·ªë c·ª•c hai c·ªôt
            col1, col2 = st.columns([1, 2])
            with col1:
                st.markdown('<div class="product-image">', unsafe_allow_html=True)
                st.image(selected_product_data['hinh_anh'], use_column_width=True)
                st.markdown('</div>', unsafe_allow_html=True)
            with col2:
                gia_ban = selected_product_data.get('gia_ban', 0)
                gia_goc = selected_product_data.get('gia_goc', 0)
                tab_info, tab_desc = st.tabs(["Th√¥ng tin chi ti·∫øt", "M√¥ t·∫£ s·∫£n ph·∫©m"])
                with tab_info:
                    st.markdown(f"**M√£ s·∫£n ph·∫©m:** {selected_product_data['ma_san_pham']}")
                    st.markdown(f"**Gi√° b√°n:** <span style='color: red; font-size: 1.2em;'>{gia_ban:,.0f} ‚Ç´</span>", unsafe_allow_html=True)
                    st.markdown(f"<span style='text-decoration: line-through; color: gray; font-size: 0.8em;'>Gi√° g·ªëc: {gia_goc:,.0f} ‚Ç´</span>", unsafe_allow_html=True)
                    # Hi·ªÉn th·ªã ƒëi·ªÉm ƒë√°nh gi√° d∆∞·ªõi d·∫°ng ng√¥i sao
                    rating = selected_product_data['diem_trung_binh']  # L·∫•y ƒëi·ªÉm ƒë√°nh gi√° trung b√¨nh
                    stars = render_stars(rating)
                    st.markdown(
                        f"**ƒêi·ªÉm ƒë√°nh gi√°:** {stars} <span style='font-size: 1.0em;'>({rating:.1f})</span>", 
                        unsafe_allow_html=True
                    )
                with tab_desc:
                    st.markdown(selected_product_data.get('mo_ta', "Kh√¥ng c√≥ m√¥ t·∫£."))

            # G·ª£i √Ω s·∫£n ph·∫©m
            recommendations = recommend_content_based(
                product_id=selected_product_data['ma_san_pham'],
                df=df_products,
                weight_content=0.7,
                weight_rating=0.3,
                top_n=7
            )
            # Lo·∫°i b·ªè s·∫£n ph·∫©m ƒë√£ ch·ªçn kh·ªèi danh s√°ch g·ª£i √Ω
            recommendations = recommendations[recommendations['ma_san_pham'] != selected_product_data['ma_san_pham']]

            st.write("### S·∫£n ph·∫©m g·ª£i √Ω:")
            cols = st.columns(3)
            for idx, (_, row) in enumerate(recommendations.iterrows()):
                col = cols[idx % 3]
                with col:
                    st.image(
                        row['hinh_anh'], 
                        use_column_width=True,  # CƒÉn ch·ªânh theo ƒë·ªô r·ªông
                        width=350  # ƒê·∫∑t chi·ªÅu r·ªông c·ªë ƒë·ªãnh
                    )
                    st.markdown(
                        f"<h4 style='font-size:18px; font-weight:bold; text-align:center;'>{row['ten_san_pham']}</h4>", 
                        unsafe_allow_html=True)
                    st.markdown(f"**M√£ s·∫£n ph·∫©m:** <span style='color: blue;'>{row.get('ma_san_pham', 'Kh√¥ng c√≥ th√¥ng tin')}</span>", unsafe_allow_html=True)

                    # L·∫•y gi√° tr·ªã v√† ƒë·ªãnh d·∫°ng gi√° b√°n
                    gia_ban = row.get('gia_ban', 'Kh√¥ng c√≥ th√¥ng tin')
                    gia_ban_formatted = f"{int(gia_ban):,}" if isinstance(gia_ban, (int, float)) and not pd.isnull(gia_ban) else gia_ban

                    # L·∫•y gi√° tr·ªã v√† ƒë·ªãnh d·∫°ng gi√° g·ªëc
                    gia_goc = row.get('gia_goc', 'Kh√¥ng c√≥ th√¥ng tin')
                    gia_goc_formatted = f"{int(gia_goc):,}" if isinstance(gia_goc, (int, float)) and not pd.isnull(gia_goc) else gia_goc

                    st.markdown(f"**Gi√° b√°n:** <span style='color: red; font-size: 1.2em;'>{gia_ban_formatted} ‚Ç´</span>", unsafe_allow_html=True)
                    st.markdown(f"<span style='text-decoration: line-through; color: gray; font-size: 0.8em;'>Gi√° g·ªëc: {gia_goc_formatted} ‚Ç´</span>", unsafe_allow_html=True)

                    # Hi·ªÉn th·ªã ƒëi·ªÉm ƒë√°nh gi√° d∆∞·ªõi d·∫°ng ng√¥i sao
                    rating = row.get('average_rating', 0)  # L·∫•y ƒëi·ªÉm ƒë√°nh gi√° trung b√¨nh
                    stars = render_stars(rating)
                    st.markdown(
                        f"**ƒêi·ªÉm ƒë√°nh gi√°:** {stars} <span style='font-size: 1.0em;'>({rating:.1f})</span>", 
                        unsafe_allow_html=True
                    )
                    with st.expander("Xem m√¥ t·∫£ s·∫£n ph·∫©m"):
                        st.write(row.get('mo_ta', "Kh√¥ng c√≥ m√¥ t·∫£."))
                    st.markdown("---")

    # Tab 2: Collaborative Filtering
    with tab2:
        @st.cache_data
        def load_customer_data(data_files):
            full_data = pd.concat([pd.read_csv(file) for file in data_files])
            full_data['ma_khach_hang'] = full_data['ma_khach_hang'].astype(str).str.strip()
            return full_data['ma_khach_hang'].unique()

        customer_ids = load_customer_data([COLLABORATIVE_FULL_DATA_PART1, COLLABORATIVE_FULL_DATA_PART2])

        # S·ª≠ d·ª•ng session_state ƒë·ªÉ l∆∞u tr·∫°ng th√°i t√™n v√† m√£ kh√°ch h√†ng
        if "customer_name" not in st.session_state:
            st.session_state.customer_name = ""
        if "customer_id" not in st.session_state:
            st.session_state.customer_id = ""

        # Nh·∫≠p th√¥ng tin kh√°ch h√†ng
        customer_name = st.text_input(
            "Nh·∫≠p t√™n c·ªßa b·∫°n:",
            value=st.session_state.customer_name
        ).strip()
        customer_id = st.selectbox(
            "Nh·∫≠p ho·∫∑c ch·ªçn m√£ kh√°ch h√†ng c·ªßa b·∫°n:",
            options=[""] + list(customer_ids),
            format_func=lambda x: f"M√£ kh√°ch h√†ng: {x}" if x else "",
            index=([""] + list(customer_ids)).index(st.session_state.customer_id) if st.session_state.customer_id in customer_ids else 0
        )

        # N√∫t ƒëƒÉng nh·∫≠p
        if st.button("ƒêƒÉng nh·∫≠p"):
            st.session_state.customer_name = customer_name  # L∆∞u t√™n v√†o session_state
            st.session_state.customer_id = customer_id     # L∆∞u m√£ v√†o session_state

        # Ch·ªâ hi·ªÉn th·ªã g·ª£i √Ω s·∫£n ph·∫©m khi ƒë√£ c√≥ th√¥ng tin
        if st.session_state.customer_name and st.session_state.customer_id:
            try:
                # Th·ª±c hi·ªán g·ª£i √Ω s·∫£n ph·∫©m
                recommendations = recommend_collaborative(
                    [COLLABORATIVE_FULL_DATA_PART1, COLLABORATIVE_FULL_DATA_PART2],
                    COLLABORATIVE_MODEL_FILE,
                    st.session_state.customer_id,
                    top_n=6
                )
                
                if not recommendations.empty:
                    st.markdown(
                        f"### C√°c s·∫£n ph·∫©m g·ª£i √Ω d√†nh ri√™ng cho <span style='color:darkgreen; font-weight:bold;'>`{st.session_state.customer_name}`</span>:",
                        unsafe_allow_html=True
                    )
                    
                    # Hi·ªÉn th·ªã s·∫£n ph·∫©m g·ª£i √Ω
                    cols = st.columns(3)  # Hi·ªÉn th·ªã l∆∞·ªõi 3 c·ªôt
                    for idx, (_, row) in enumerate(recommendations.iterrows()):
                        col = cols[idx % 3]
                        with col:
                            st.image(row['hinh_anh'], use_column_width=True)
                            st.markdown(f"<h4 style='font-size:18px; font-weight:bold; text-align:center;'>{row['ten_san_pham']}</h4>", 
                                        unsafe_allow_html=True)
                            st.markdown(f"**M√£ s·∫£n ph·∫©m:** <span style='color: blue;'>{row.get('ma_san_pham', 'Kh√¥ng c√≥ th√¥ng tin')}</span>", unsafe_allow_html=True)
                            gia_ban = row.get('gia_ban', 0)
                            gia_goc = row.get('gia_goc', 0)
                            mo_ta = row.get('mo_ta', "Kh√¥ng c√≥ m√¥ t·∫£.")
                            st.markdown(f"<strong>Gi√° b√°n:</strong> <span style='color: red; font-size: 1.2em;'>{gia_ban:,.0f} ‚Ç´</span>", unsafe_allow_html=True)
                            st.markdown(f"<span style='text-decoration: line-through; color: gray; font-size: 0.8em;'>Gi√° g·ªëc: {gia_goc:,.0f} ‚Ç´</span>", unsafe_allow_html=True)
                            rating = row.get('diem_trung_binh', 0)  # L·∫•y ƒëi·ªÉm ƒë√°nh gi√° trung b√¨nh
                            stars = render_stars(rating)
                            st.markdown(
                                f"**ƒêi·ªÉm ƒë√°nh gi√°:** {stars} <span style='font-size: 1.0em;'>({rating:.1f})</span>", 
                                unsafe_allow_html=True
                            )
                            with st.expander("Xem m√¥ t·∫£ s·∫£n ph·∫©m"):
                                st.write(f"{mo_ta}")
                            st.markdown("---")
                else:
                    st.warning("Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m g·ª£i √Ω ph√π h·ª£p.")
            except Exception as e:
                st.error(f"ƒê√£ x·∫£y ra l·ªói: {e}")
        elif st.session_state.customer_name and not st.session_state.customer_id:
            st.warning("Vui l√≤ng nh·∫≠p ho·∫∑c ch·ªçn m√£ kh√°ch h√†ng ƒë·ªÉ g·ª£i √Ω s·∫£n ph·∫©m!")
        elif not st.session_state.customer_name and st.session_state.customer_id:
            st.warning("Vui l√≤ng nh·∫≠p t√™n c·ªßa b·∫°n!")