import streamlit as st
import pandas as pd
from collaborative_recommend import recommend_products as recommend_collaborative
from content_based_recommendation import recommend_products as recommend_content_based

# ƒê∆∞·ªùng d·∫´n t·ªáp
CONTENT_BASED_DATA_FILE = "data/content_based_preprocessed.csv"
COLLABORATIVE_FULL_DATA_PART1 = "data/collaborative_full_data_part1.csv"
COLLABORATIVE_FULL_DATA_PART2 = "data/collaborative_full_data_part2.csv"
COLLABORATIVE_MODEL_FILE = "model/collaborative_model.pkl.gz"

# Set page configuration
st.set_page_config(
    page_title="Hasaki Recommendation System",
    page_icon="üíÑ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Custom CSS for Hasaki-themed design
st.markdown("""
    <style>
    /* Overall page background */
    body {
        background-color: #f0f4f0;
        color: #2f6e51;
    }

    /* Sidebar styling */
    [data-testid="stSidebar"] {
        background-color: #e6f2e8;
        border-right: 2px solid #2f6e51;
    }

    /* Header and title styles */
    .header-title {
        color: #2f6e51;
        font-family: 'Arial', sans-serif;
        text-align: center;
        padding: 20px;
        background-color: #d0e5d3;
        border-radius: 10px;
        margin-bottom: 20px;
    }

    /* Banner Styling */
    .banner {
        background: url('banner/hasaki_banner_2.jpg') no-repeat center center;
        background-size: cover;
        height: 150px; /* Adjust height as needed */
        display: flex;
        justify-content: center;
        align-items: center;
        color: white;
        font-size: 32px;
        font-weight: bold;
        text-shadow: 2px 2px 5px rgba(0,0,0,0.5);
    }

    /* Layout for product display */
    .product-layout {
        display: flex;
        justify-content: space-between;
        gap: 20px;
        margin-top: 20px;
    }

    /* Style for product image */
    .product-image img {
        max-width: 100%;
        height: auto;
        border-radius: 10px;
        box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);
    }
    </style>
""", unsafe_allow_html=True)

# Sidebar th√¥ng tin nh√≥m
st.sidebar.title("Th√¥ng tin nh√≥m th·ª±c hi·ªán")
st.sidebar.write("""#### Th√†nh vi√™n th·ª±c hi·ªán:
- L√™ Qu·ª≥nh Minh Dung
- Nguy·ªÖn Th√πy Trang""")
st.sidebar.write("""#### Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n: Khu·∫•t Th√πy Ph∆∞∆°ng""")
st.sidebar.write("""#### Th·ªùi gian th·ª±c hi·ªán: 12/2024""")

st.sidebar.title("Navigation")
page = st.sidebar.radio(
    "Select a page:", 
    ["Introduction", "Recommendation Process", "Recommendation System"]
)

# Page 1: Introduction
if page == "Introduction":
    # Hi·ªÉn th·ªã banner
    st.title("Ch√†o m·ª´ng ƒë·∫øn v·ªõi Hasaki.vn!")
    
    col1, col2 = st.columns([1, 3])  # T·∫°o hai c·ªôt, c·ªôt ƒë·∫ßu nh·ªè h∆°n ƒë·ªÉ ch·ª©a logo

    # C·ªôt 1: Hi·ªÉn th·ªã logo
    with col1:
        st.image("banner/Logo.png", use_column_width=False, width=200, caption="")

    # C·ªôt 2: Hi·ªÉn th·ªã ph·∫ßn "V·ªÅ Hasaki"
    with col2:
        st.subheader("V·ªÅ Hasaki.vn üíÑ")
        st.write("""
            Hasaki.vn cam k·∫øt mang ƒë·∫øn nh·ªØng s·∫£n ph·∫©m l√†m ƒë·∫πp v√† chƒÉm s√≥c da t·ªët nh·∫•t cho kh√°ch h√†ng.
            V·ªõi tr·ªçng t√¢m l√† ch·∫•t l∆∞·ª£ng v√† s·ª± h√†i l√≤ng c·ªßa kh√°ch h√†ng, Hasaki h∆∞·ªõng ƒë·∫øn vi·ªác l√†m ƒë·∫πp tr·ªü n√™n d·ªÖ ti·∫øp c·∫≠n v·ªõi m·ªçi ng∆∞·ªùi.
        """)

    st.subheader("üéØ M·ª•c ti√™u ch√≠nh")
    st.write("""
        H·ªá th·ªëng g·ª£i √Ω (Recommend system) c·ªßa ch√∫ng t√¥i ƒë∆∞·ª£c thi·∫øt k·∫ø nh·∫±m:
        - üõçÔ∏è Gi√∫p kh√°ch h√†ng c·ªßa ch√∫ng t√¥i kh√°m ph√° c√°c s·∫£n ph·∫©m ph√π h·ª£p v·ªõi s·ªü th√≠ch c·ªßa m√¨nh.
        - üí° C·∫£i thi·ªán tr·∫£i nghi·ªám mua s·∫Øm b·∫±ng c√°ch g·ª£i √Ω c√°c s·∫£n ph·∫©m li√™n quan.
        - üìä ·ª®ng d·ª•ng c√°c thu·∫≠t to√°n ti√™n ti·∫øn nh∆∞ Content-Based Filtering v√† Collaborative Filtering ƒë·ªÉ mang l·∫°i g·ª£i √Ω c√° nh√¢n h√≥a.
    """)
    st.markdown("<br>", unsafe_allow_html=True)  # Th√™m kho·∫£ng tr·ªëng
    st.image("banner/images.png", use_column_width=True, caption="Mang ƒë·∫øn tr·∫£i nghi·ªám l√†m ƒë·∫πp tuy·ªát v·ªùi")

# Page 2: Recommendation Process
elif page == "Recommendation Process":
    st.title("Quy tr√¨nh x√¢y d·ª±ng Recommendation System")
    st.write("""
        Quy tr√¨nh x√¢y d·ª±ng h·ªá th·ªëng g·ª£i √Ω t·∫°i Hasaki.vn ƒë∆∞·ª£c chia th√†nh hai ph∆∞∆°ng ph√°p ch√≠nh:
        1. **Content-Based Filtering**: G·ª£i √Ω s·∫£n ph·∫©m d·ª±a tr√™n n·ªôi dung m√¥ t·∫£ c·ªßa s·∫£n ph·∫©m.
        2. **Collaborative Filtering**: G·ª£i √Ω s·∫£n ph·∫©m d·ª±a tr√™n h√†nh vi c·ªßa kh√°ch h√†ng kh√°c.
    """)

    tab1, tab2, tab3 = st.tabs(["Crawl Data", "Content-Based Filtering", "Collaborative Filtering"])

    # Tab Crawl Data
    with tab1:
        st.subheader("Crawl Data t·ª´ Hasaki.vn")

        with st.expander("Code c√†o d·ªØ li·ªáu (Click ƒë·ªÉ m·ªü)"):
            st.code("""
    from bs4 import BeautifulSoup
    import requests
    import pandas as pd
    import time

    # URL c·ªßa trang web "ChƒÉm s√≥c da m·∫∑t" tr√™n Hasaki.vn
    BASE_URL = "https://hasaki.vn/danh-muc/cham-soc-da-mat-c4.html"

    # H√†m c√†o d·ªØ li·ªáu ch√≠nh
    def scrape_hasaki_data(base_url, num_pages=5):
        products = []
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36",
            "Accept-Language": "en-US,en;q=0.9",
        }

        for page in range(1, num_pages + 1):
            url = f"{base_url}?p={page}"  # C·∫≠p nh·∫≠t tham s·ªë trang
            print(f"ƒêang c√†o d·ªØ li·ªáu t·ª´: {url}")
            try:
                response = requests.get(url, headers=headers)
                if response.status_code != 200:
                    print(f"Kh√¥ng th·ªÉ truy c·∫≠p trang: {url} (status code: {response.status_code})")
                    continue
                
                soup = BeautifulSoup(response.content, 'html.parser')
                product_items = soup.find_all('div', class_='item_sp_hasaki width_common relative')

                for item in product_items:
                    try:
                        # L·∫•y th√¥ng tin t·ª´ HTML
                        product_link_tag = item.find('a', class_='block_info_item_sp width_common card-body')
                        product_id = product_link_tag['data-id'] if product_link_tag and 'data-id' in product_link_tag.attrs else "Kh√¥ng c√≥ m√£ s·∫£n ph·∫©m"
                        product_name = product_link_tag['data-name'] if product_link_tag and 'data-name' in product_link_tag.attrs else "Kh√¥ng c√≥ t√™n s·∫£n ph·∫©m"
                        price = item.find('strong', class_='item_giamoi txt_16').text.strip().replace(" ‚Ç´", "").replace(".", "") if item.find('strong', class_='item_giamoi txt_16') else "Kh√¥ng c√≥ gi√° b√°n"
                        original_price = item.find('span', class_='item_giacu txt_12 right').text.strip().replace(" ‚Ç´", "").replace(".", "") if item.find('span', class_='item_giacu txt_12 right') else "Kh√¥ng c√≥ gi√° g·ªëc"
                        image_url = item.find('img', class_='img_thumb lazy')['data-src'] if item.find('img', class_='img_thumb lazy') else "Kh√¥ng c√≥ h√¨nh ·∫£nh"

                        # Th√™m s·∫£n ph·∫©m v√†o danh s√°ch
                        products.append({
                            "ma_san_pham": product_id,
                            "ten_san_pham": product_name,
                            "gia_ban": price,
                            "gia_goc": original_price,
                            "hinh_anh": image_url,
                        })
                    except Exception as e:
                        print(f"L·ªói khi x·ª≠ l√Ω s·∫£n ph·∫©m: {e}")

                time.sleep(2)  # Ngh·ªâ 2 gi√¢y gi·ªØa c√°c y√™u c·∫ßu

            except Exception as e:
                print(f"L·ªói khi truy c·∫≠p: {url}, {e}")
        
        return products

        # G·ªçi h√†m c√†o d·ªØ li·ªáu
        data = scrape_hasaki_data(BASE_URL, num_pages=68)

        # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√†nh DataFrame
        df = pd.DataFrame(data)

        # Lo·∫°i b·ªè s·∫£n ph·∫©m tr√πng l·∫∑p d·ª±a tr√™n 'ma_san_pham'
        df = df.drop_duplicates(subset=['ma_san_pham'], keep='first')

        # L∆∞u th√†nh file CSV
        df.to_csv('San_pham_new.csv', index=False, encoding='utf-8-sig')
        print("D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o file San_pham_new.csv!")
                """, language="python")

        st.write("""
            **Gi·∫£i th√≠ch:**
            - **M·ª•c ƒë√≠ch:** C√†o d·ªØ li·ªáu s·∫£n ph·∫©m t·ª´ danh m·ª•c "ChƒÉm s√≥c da m·∫∑t" tr√™n trang Hasaki.vn.
            - **Chi ti·∫øt:** 
                1. **`BeautifulSoup`**: Th∆∞ vi·ªán ƒë·ªÉ ph√¢n t√≠ch c√∫ ph√°p HTML.
                2. **D·ªØ li·ªáu c√†o ƒë∆∞·ª£c:**
                    - **`ma_san_pham`**: M√£ ƒë·ªãnh danh s·∫£n ph·∫©m.
                    - **`ten_san_pham`**: T√™n s·∫£n ph·∫©m.
                    - **`gia_ban`**: Gi√° hi·ªán t·∫°i c·ªßa s·∫£n ph·∫©m.
                    - **`gia_goc`**: Gi√° g·ªëc (n·∫øu c√≥).
                    - **`hinh_anh`**: URL h√¨nh ·∫£nh s·∫£n ph·∫©m.
                3. **Lo·∫°i b·ªè tr√πng l·∫∑p:** S·∫£n ph·∫©m ƒë∆∞·ª£c l·ªçc d·ª±a tr√™n `ma_san_pham`.
                4. **ƒê·∫ßu ra:** L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o file San_pham_new.csv ƒë·ªÉ merge chung v·ªõi file g·ªëc l√† San_pham.csv d·ª±a tr√™n c·ªôt `ma_san_pham`.
                5. **Th·ªùi gian ngh·ªâ (sleep):** Gi·ªØa m·ªói l·∫ßn c√†o m·ªôt trang, ch∆∞∆°ng tr√¨nh ngh·ªâ 2 gi√¢y ƒë·ªÉ tr√°nh b·ªã ch·∫∑n.
        """)

        st.write("### D·ªØ li·ªáu m·∫´u c√†o ƒë∆∞·ª£c:")
        # Hi·ªÉn th·ªã d·ªØ li·ªáu m·∫´u
        try:
            df_sample = pd.read_csv('data/San_pham_new.csv').head()
            st.dataframe(df_sample)
        except FileNotFoundError:
            st.warning("Kh√¥ng t√¨m th·∫•y file `San_pham_new.csv`. Vui l√≤ng ch·∫°y m√£ c√†o d·ªØ li·ªáu tr∆∞·ªõc.")


    # Tab Content-Based Filtering
    with tab2:
        # Streamlit layout
        st.title("Content-Based Filtering: Preprocessing and Analysis")

        st.write("### C√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω:")
        st.markdown("""
        1. **ƒê·ªçc d·ªØ li·ªáu t·ª´ c√°c t·ªáp CSV:**
            - **San_pham.csv:** Ch·ª©a th√¥ng tin s·∫£n ph·∫©m (t√™n, m√¥ t·∫£, gi√°, ƒëi·ªÉm ƒë√°nh gi√°...).
            - **Danh_gia.csv:** Ch·ª©a ƒë√°nh gi√° c·ªßa kh√°ch h√†ng.
        2. **Lo·∫°i b·ªè stopwords v√† l√†m s·∫°ch d·ªØ li·ªáu (Tonkenize).**
        3. **T√≠nh to√°n ƒë·∫∑c tr∆∞ng:**
            - T·∫°o n·ªôi dung phong ph√∫ h∆°n b·∫±ng c√°ch k·∫øt h·ª£p t√™n, m√¥ t·∫£ s·∫£n ph·∫©m v√† ph√¢n lo·∫°i ƒë·ªÉ t·∫°o n·ªôi dung phong ph√∫ h∆°n.
            - T√≠nh to√°n ƒëi·ªÉm t·ªïng h·ª£p cho t·ª´ng s·∫£n ph·∫©m b·∫±ng c√°ch k·∫øt h·ª£p ƒëi·ªÉm t∆∞∆°ng t·ª± n·ªôi dung (similarity score) v√† ƒëi·ªÉm ƒë√°nh gi√° trung b√¨nh (average rating). D√πng ƒëi·ªÉm t·ªïng h·ª£p n√†y ƒë·ªÉ x√°c ƒë·ªãnh c√°c s·∫£n ph·∫©m t∆∞∆°ng t·ª± nh·∫•t nh∆∞ng v·∫´n ƒë·∫£m b·∫£o ∆∞u ti√™n c√°c s·∫£n ph·∫©m c√≥ ƒë√°nh gi√° t·ªët h∆°n.
        """)

        # Load d·ªØ li·ªáu
        san_pham_path = "data/san_pham_updated.csv"
        san_pham_preprocessed_path = "data/content_based_preprocessed.csv"
        san_pham_df = pd.read_csv(san_pham_path)
        san_pham_preprocessed_df = pd.read_csv(san_pham_preprocessed_path)

        # Display raw data
        st.write("### D·ªØ li·ªáu s·∫£n ph·∫©m g·ªëc:")
        st.dataframe(san_pham_df[['ma_san_pham', 'ten_san_pham', 'mo_ta', 'diem_trung_binh']].head())

        # Display tokenized data
        st.write("### D·ªØ li·ªáu sau khi ti·ªÅn x·ª≠ l√Ω:")
        st.dataframe(san_pham_preprocessed_df[['ma_san_pham', 'processed_content', 'tokens', 'token_count']].head())

        # Token distribution
        st.write("### Ph√¢n ph·ªëi s·ªë l∆∞·ª£ng t·ª´ trong m√¥ t·∫£ s·∫£n ph·∫©m:")
        st.image("banner/distribution.png", use_column_width=True, caption="")

        st.markdown("""
        **Nh·∫≠n x√©t:**
        - ƒê·ªô d√†i m√¥ t·∫£ t·∫≠p trung trong kho·∫£ng 100-150 t·ª´, cho th·∫•y m√¥ t·∫£ s·∫£n ph·∫©m ·ªü m·ª©c ƒë·ªô v·ª´a ph·∫£i, kh√¥ng qu√° d√†i ho·∫∑c ng·∫Øn.
        - M·ªôt s·ªë s·∫£n ph·∫©m c√≥ m√¥ t·∫£ r·∫•t ng·∫Øn (<100 tokens) ho·∫∑c r·∫•t d√†i (>300 tokens), c√≥ th·ªÉ c·∫ßn ƒë∆∞·ª£c chu·∫©n h√≥a.
        - Ph√¢n ph·ªëi ƒë·ªÅu, kh√¥ng c√≥ s·ª± l·ªách ƒë√°ng k·ªÉ.
        """)

        # Relationship between token count and average rating
        st.write("### Quan h·ªá gi·ªØa ƒëi·ªÉm ƒë√°nh gi√° v√† ƒë·ªô d√†i m√¥ t·∫£:")
        st.image("banner/relationship.png", use_column_width=True, caption="")

        st.markdown("""
        **Nh·∫≠n x√©t:**
        - Kh√¥ng c√≥ m·ªëi quan h·ªá tuy·∫øn t√≠nh r√µ r√†ng gi·ªØa ƒë·ªô d√†i m√¥ t·∫£ (token_count) v√† ƒëi·ªÉm ƒë√°nh gi√° trung b√¨nh (diem_trung_binh).
        - C√°c s·∫£n ph·∫©m c√≥ ƒëi·ªÉm cao (4-5) xu·∫•t hi·ªán ·ªü nhi·ªÅu m·ª©c token, t·ª´ ng·∫Øn ƒë·∫øn d√†i.
        - C√°c s·∫£n ph·∫©m c√≥ ƒëi·ªÉm b·∫±ng 0 tr·∫£i r·ªông ·ªü m·ªçi ƒë·ªô d√†i m√¥ t·∫£, c·∫ßn ki·ªÉm tra l·∫°i d·ªØ li·ªáu.
        """)

        # Correlation heatmap
        st.write("### Heatmap t∆∞∆°ng quan:")
        st.image("banner/heatmap.png", use_column_width=True, caption="")

        st.markdown("""
        **Nh·∫≠n x√©t:**
        - T∆∞∆°ng quan gi·ªØa diem_trung_binh v√† token_count l√† -0.02 (g·∫ßn 0), cho th·∫•y ƒë·ªô d√†i m√¥ t·∫£ kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn ƒëi·ªÉm ƒë√°nh gi√°.
        - C·∫ßn ph√¢n t√≠ch th√™m c√°c y·∫øu t·ªë kh√°c (gi√° c·∫£, lo·∫°i s·∫£n ph·∫©m, h√¨nh ·∫£nh) ƒë·ªÉ t√¨m m·ªëi quan h·ªá c√≥ √Ω nghƒ©a h∆°n.
        """)

        # Top 10 most frequent words
        st.write("### Top 10 t·ª´ ph·ªï bi·∫øn nh·∫•t trong m√¥ t·∫£ s·∫£n ph·∫©m:")
        st.image("banner/frequent_words.png", use_column_width=True, caption="")

        st.markdown("""
        **Nh·∫≠n x√©t:**
        - "h√†ng", "ƒë∆°n", "da", "h√≥a": ƒê√¢y l√† c√°c t·ª´ ph·ªï bi·∫øn nh∆∞ng mang t√≠nh t·ªïng qu√°t v√† th∆∞·ªùng kh√¥ng cung c·∫•p nhi·ªÅu th√¥ng tin ƒë·∫∑c tr∆∞ng cho s·∫£n ph·∫©m.
        - "ƒë·ªè", "hasaki": C√°c t·ª´ n√†y c√≥ th·ªÉ l√† ƒë·∫∑c tr∆∞ng c·ªßa s·∫£n ph·∫©m (m√†u s·∫Øc ho·∫∑c th∆∞∆°ng hi·ªáu) nh∆∞ng c·∫ßn ki·ªÉm tra xem nh·ªØng t·ª´ n√†y c√≥ xu·∫•t hi·ªán qu√° th∆∞·ªùng xuy√™n v√† c√≥ gi√° tr·ªã ph√¢n t√≠ch hay kh√¥ng.
        - "kh√¥ng", "xu·∫•t", "h": ƒê√¢y l√† nh·ªØng t·ª´ c√≥ kh·∫£ nƒÉng kh√¥ng mang l·∫°i √Ω nghƒ©a ƒë·∫∑c bi·ªát cho m√¥ t·∫£ s·∫£n ph·∫©m, ƒë·∫∑c bi·ªát t·ª´ nh∆∞ "h" ho·∫∑c "kh√¥ng" c√≥ th·ªÉ b·ªã xem l√† stopwords.
        """)

        # Tab Collaborative Filtering
        with tab3:
            st.subheader("Collaborative Filtering")
            st.write("### B∆∞·ªõc 1: X·ª≠ l√Ω d·ªØ li·ªáu kh√°ch h√†ng v√† s·∫£n ph·∫©m")
            st.code("""
            import pandas as pd

            def preprocess_data(input_file, output_file):
                # Code x·ª≠ l√Ω d·ªØ li·ªáu Collaborative Filtering
                pass
                    """, language="python")
            st.write("### B∆∞·ªõc 2: Hu·∫•n luy·ªán m√¥ h√¨nh Collaborative Filtering")
            st.code("""
            from surprise import Dataset, KNNBaseline

            def train_model(data_file, model_file):
                # Code hu·∫•n luy·ªán Collaborative Filtering
                pass
                    """, language="python")

# Page 3: Recommendation System
elif page == "Recommendation System":
    # Hi·ªÉn th·ªã banner
    banner_path = "banner/hasaki_banner.png"  # ƒê∆∞·ªùng d·∫´n c·ª•c b·ªô

    st.image(
        banner_path,
        use_column_width=True,  # T·ª± ƒë·ªông cƒÉn ch·ªânh theo chi·ªÅu r·ªông c·ªßa trang
        caption=None  # Kh√¥ng hi·ªÉn th·ªã ch√∫ th√≠ch
    )
    # Hi·ªÉn th·ªã ti√™u ƒë·ªÅ ch√≠nh
    st.title("Hasaki g·ª£i √Ω s·∫£n ph·∫©m cho b·∫°n")

    # Tabs ƒë·ªÉ ch·ªçn gi·ªØa hai ph∆∞∆°ng ph√°p g·ª£i √Ω
    tab1, tab2 = st.tabs(["Content-Based Filtering", "Collaborative Filtering"])

    # Tab 1: Content-Based Filtering
    with tab1:
        st.subheader("Content-based filtering")

        # ƒê·ªçc d·ªØ li·ªáu s·∫£n ph·∫©m
        df_products = pd.read_csv(CONTENT_BASED_DATA_FILE)
        df_products['ten_san_pham'] = df_products['ten_san_pham'].astype(str).fillna('')
        df_products['ma_san_pham'] = df_products['ma_san_pham'].astype(str).fillna('')
        df_products['hinh_anh'] = df_products['hinh_anh'].astype(str).fillna("https://via.placeholder.com/150")

        # Ch·ªçn t√™n s·∫£n ph·∫©m
        product_names = df_products['ten_san_pham'].unique()
        selected_product_name = st.selectbox(
            "Nh·∫≠p ho·∫∑c ch·ªçn t√™n s·∫£n ph·∫©m y√™u th√≠ch:",
            options=[""] + list(product_names),
            format_func=lambda x: x if x else "Ch·ªçn s·∫£n ph·∫©m",
            key="product_name"
        )

        # L·ªçc s·∫£n ph·∫©m d·ª±a tr√™n l·ª±a ch·ªçn
        filtered_products = df_products[df_products['ten_san_pham'] == selected_product_name] if selected_product_name else pd.DataFrame()

        # Hi·ªÉn th·ªã s·∫£n ph·∫©m ƒë√£ ch·ªçn
        if not filtered_products.empty:
            selected_product_data = filtered_products.iloc[0]
            st.markdown("---")
            st.markdown(f"<h2 style='color: #2E8B57; text-align: center;'>{selected_product_data['ten_san_pham']}</h2>", unsafe_allow_html=True)
            
            # T·∫°o b·ªë c·ª•c hai c·ªôt
            col1, col2 = st.columns([1, 2])
            with col1:
                st.markdown('<div class="product-image">', unsafe_allow_html=True)
                st.image(selected_product_data['hinh_anh'], use_column_width=True)
                st.markdown('</div>', unsafe_allow_html=True)
            with col2:
                gia_ban = selected_product_data.get('gia_ban', 0)
                gia_goc = selected_product_data.get('gia_goc', 0)
                tab_info, tab_desc = st.tabs(["Th√¥ng tin chi ti·∫øt", "M√¥ t·∫£ s·∫£n ph·∫©m"])
                with tab_info:
                    st.markdown(f"**M√£ s·∫£n ph·∫©m:** {selected_product_data['ma_san_pham']}")
                    st.markdown(f"**Gi√° b√°n:** <span style='color: red;'>{gia_ban:,.0f} ‚Ç´</span>", unsafe_allow_html=True)
                    st.markdown(f"<span style='text-decoration: line-through; color: gray;'>Gi√° g·ªëc: {gia_goc:,.0f} ‚Ç´</span>", unsafe_allow_html=True)
                    st.markdown(f"**ƒêi·ªÉm ƒë√°nh gi√°:** {selected_product_data.get('diem_trung_binh', 'Kh√¥ng c√≥ th√¥ng tin')}")
                with tab_desc:
                    st.markdown(selected_product_data.get('mo_ta', "Kh√¥ng c√≥ m√¥ t·∫£."))

            # G·ª£i √Ω s·∫£n ph·∫©m
            recommendations = recommend_content_based(
                product_id=selected_product_data['ma_san_pham'],
                df=df_products,
                weight_content=0.7,
                weight_rating=0.3,
                top_n=7
            )
            # Lo·∫°i b·ªè s·∫£n ph·∫©m ƒë√£ ch·ªçn kh·ªèi danh s√°ch g·ª£i √Ω
            recommendations = recommendations[recommendations['ma_san_pham'] != selected_product_data['ma_san_pham']]

            st.write("### S·∫£n ph·∫©m g·ª£i √Ω:")
            cols = st.columns(3)
            for idx, (_, row) in enumerate(recommendations.iterrows()):
                col = cols[idx % 3]
                with col:
                    st.image(
                        row['hinh_anh'], 
                        caption=row['ten_san_pham'], 
                        use_column_width=False,  # T·∫Øt t·ª± ƒë·ªông cƒÉn ch·ªânh
                        width=350  # ƒê·∫∑t chi·ªÅu r·ªông c·ªë ƒë·ªãnh
                    )
                    st.markdown(f"**M√£ s·∫£n ph·∫©m:** <span style='color: blue;'>{row.get('ma_san_pham', 'Kh√¥ng c√≥ th√¥ng tin')}</span>", unsafe_allow_html=True)
                    st.markdown(f"**Gi√° b√°n:** <span style='color: red;'>{row.get('gia_ban', 'Kh√¥ng c√≥ th√¥ng tin')} ‚Ç´</span>", unsafe_allow_html=True)
                    st.markdown(f"<span style='text-decoration: line-through; color: gray;'>Gi√° g·ªëc: {row.get('gia_goc', 'Kh√¥ng c√≥ th√¥ng tin')} ‚Ç´</span>", unsafe_allow_html=True)
                    st.markdown(f"**ƒêi·ªÉm ƒë√°nh gi√°:** {row.get('average_rating', 'Kh√¥ng c√≥ th√¥ng tin')}")
                    with st.expander("Xem m√¥ t·∫£ s·∫£n ph·∫©m"):
                        st.write(row.get('mo_ta', "Kh√¥ng c√≥ m√¥ t·∫£."))
                    st.markdown("---")

    # Tab 2: Collaborative Filtering
    with tab2:
        st.subheader("Collaborative filtering")

        @st.cache_data
        def load_customer_data(data_files):
            full_data = pd.concat([pd.read_csv(file) for file in data_files])
            full_data['ma_khach_hang'] = full_data['ma_khach_hang'].astype(str).str.strip()
            return full_data['ma_khach_hang'].unique()

        customer_ids = load_customer_data([COLLABORATIVE_FULL_DATA_PART1, COLLABORATIVE_FULL_DATA_PART2])

        # S·ª≠ d·ª•ng session_state ƒë·ªÉ l∆∞u tr·∫°ng th√°i t√™n v√† m√£ kh√°ch h√†ng
        if "customer_name" not in st.session_state:
            st.session_state.customer_name = ""
        if "customer_id" not in st.session_state:
            st.session_state.customer_id = ""

        # Nh·∫≠p th√¥ng tin kh√°ch h√†ng
        customer_name = st.text_input(
            "Nh·∫≠p t√™n c·ªßa b·∫°n:",
            value=st.session_state.customer_name
        ).strip()
        customer_id = st.selectbox(
            "Nh·∫≠p ho·∫∑c ch·ªçn m√£ kh√°ch h√†ng c·ªßa b·∫°n:",
            options=[""] + list(customer_ids),
            format_func=lambda x: f"M√£ kh√°ch h√†ng: {x}" if x else "",
            index=([""] + list(customer_ids)).index(st.session_state.customer_id) if st.session_state.customer_id in customer_ids else 0
        )

        # N√∫t ƒëƒÉng nh·∫≠p
        if st.button("ƒêƒÉng nh·∫≠p"):
            st.session_state.customer_name = customer_name  # L∆∞u t√™n v√†o session_state
            st.session_state.customer_id = customer_id     # L∆∞u m√£ v√†o session_state

        # Ch·ªâ hi·ªÉn th·ªã g·ª£i √Ω s·∫£n ph·∫©m khi ƒë√£ c√≥ th√¥ng tin
        if st.session_state.customer_name and st.session_state.customer_id:
            try:
                # Th·ª±c hi·ªán g·ª£i √Ω s·∫£n ph·∫©m
                recommendations = recommend_collaborative(
                    [COLLABORATIVE_FULL_DATA_PART1, COLLABORATIVE_FULL_DATA_PART2],
                    COLLABORATIVE_MODEL_FILE,
                    st.session_state.customer_id,
                    top_n=6
                )
                
                if not recommendations.empty:
                    st.markdown(
                        f"### C√°c s·∫£n ph·∫©m g·ª£i √Ω d√†nh ri√™ng cho <span style='color:darkgreen; font-style:italic;'>{st.session_state.customer_name}</span>:",
                        unsafe_allow_html=True
                    )
                    
                    # Hi·ªÉn th·ªã s·∫£n ph·∫©m g·ª£i √Ω
                    cols = st.columns(3)  # Hi·ªÉn th·ªã l∆∞·ªõi 3 c·ªôt
                    for idx, (_, row) in enumerate(recommendations.iterrows()):
                        col = cols[idx % 3]
                        with col:
                            st.image(row['hinh_anh'], use_column_width=True, caption=row['ten_san_pham'])
                            gia_ban = row.get('gia_ban', 0)
                            gia_goc = row.get('gia_goc', 0)
                            mo_ta = row.get('mo_ta', "Kh√¥ng c√≥ m√¥ t·∫£.")
                            st.markdown(f"**Gi√° b√°n:** {gia_ban:,.0f} ‚Ç´")
                            st.markdown(f"<span style='text-decoration: line-through; color: gray; font-size: 0.8em;'>Gi√° g·ªëc: {gia_goc:,.0f} ‚Ç´</span>", unsafe_allow_html=True)
                            st.write(f"**ƒêi·ªÉm ƒë√°nh gi√° trung b√¨nh:** {row['diem_trung_binh']:.2f}")
                            with st.expander("Xem m√¥ t·∫£ s·∫£n ph·∫©m"):
                                st.write(f"{mo_ta}")
                            st.markdown("---")
                else:
                    st.warning("Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m g·ª£i √Ω ph√π h·ª£p.")
            except Exception as e:
                st.error(f"ƒê√£ x·∫£y ra l·ªói: {e}")
        elif st.session_state.customer_name and not st.session_state.customer_id:
            st.warning("Vui l√≤ng nh·∫≠p ho·∫∑c ch·ªçn m√£ kh√°ch h√†ng ƒë·ªÉ g·ª£i √Ω s·∫£n ph·∫©m!")
        elif not st.session_state.customer_name and st.session_state.customer_id:
            st.warning("Vui l√≤ng nh·∫≠p t√™n c·ªßa b·∫°n!")